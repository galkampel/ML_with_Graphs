{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPGRFY2dqq+xbwJ8SwAdrW1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3aMDfWzdrbti"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["# installation"],"metadata":{"id":"IvSRlj0gRbNK"}},{"cell_type":"code","source":["# Install torch geometric\n","import os\n","if 'IS_GRADESCOPE_ENV' not in os.environ:\n","  !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","  !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","  !pip install torch-geometric\n","  !pip install -q git+https://github.com/snap-stanford/deepsnap.git\n","  !pip install -U -q PyDrive"],"metadata":{"id":"iV4l8-CNReM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'IS_GRADESCOPE_ENV' not in os.environ:\n","  !nvcc --version\n","  !python -c \"import torch; print(torch.version.cuda)\""],"metadata":{"id":"o8-Vl-LcRh9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  import torch\n","  print(torch.__version__)\n","  import torch_geometric\n","  print(torch_geometric.__version__)"],"metadata":{"id":"hF0AwvfhRXt3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","import torch\n","import deepsnap\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch_geometric.nn as pyg_nn\n","\n","from sklearn.metrics import f1_score\n","from deepsnap.hetero_gnn import forward_op\n","from deepsnap.hetero_graph import HeteroGraph\n","from torch_sparse import SparseTensor, matmul"],"metadata":{"id":"bw_eAQEDRQB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset\n","You need to login to your Google account and enter the verification code below."],"metadata":{"id":"TbiCLigJRIzF"}},{"cell_type":"code","source":["if 'IS_GRADESCOPE_ENV' not in os.environ:\n","  from pydrive.auth import GoogleAuth\n","  from pydrive.drive import GoogleDrive\n","  from google.colab import auth\n","  from oauth2client.client import GoogleCredentials\n","\n","  # Authenticate and create the PyDrive client\n","  auth.authenticate_user()\n","  gauth = GoogleAuth()\n","  gauth.credentials = GoogleCredentials.get_application_default()\n","  drive = GoogleDrive(gauth)"],"metadata":{"id":"cDIuhGGNRLiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'IS_GRADESCOPE_ENV' not in os.environ:\n","  id='1ivlxd6lJMcZ9taS44TMGG72x2V1GeVvk'\n","  downloaded = drive.CreateFile({'id': id})\n","  downloaded.GetContentFile('acm.pkl')"],"metadata":{"id":"o_QMD8m-RNc5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# HeteroGNN model"],"metadata":{"id":"J6tU0wq2rfMX"}},{"cell_type":"markdown","source":["## Implementing `HeteroGNNConv`\n","\n","Now let's start working on our own implementation of the heterogeneous message passing layer (`HeteroGNNConv`)! Just as in Colabs 3 and 4, we will implement the layer using PyTorch Geometric. \n","\n","At a high level, the `HeteroGNNConv` layer is equivalent to the homogenous GNN layers we implemented in Colab 3, but now applied to an individual heterogeous message type. Moreover, our heterogeneous GNN layer draws directly from the **GraphSAGE** message passing model ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)).\n","\n","We begin by defining the `HeteroGNNConv` layer with respect to message type $m$:\n","\n","\\begin{equation}\n","m =(s, r, d)\n","\\end{equation}\n","\n","where each message type is a tuple containing three elements: $s$ - the source node type, $r$ - the edge (relation) type, and $d$ - the destination node type. \n","\n","The message passing update rule that we implement is very similar to that of GraphSAGE, except we now need to include the node types and the edge relation type. The update rule for message type $m$ is described below:\n","\n","\\begin{equation}\n","h_v^{(l)[m]} = W^{(l)[m]} \\cdot \\text{CONCAT} \\Big( W_d^{(l)[m]} \\cdot h_v^{(l-1)}, W_s^{(l)[m]} \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N_{m}(v) \\})\\Big)\n","\\end{equation}\n","\n","where we compute $h_v^{(l)[m]}$, the node embedding representation for node $v$ after `HeteroGNNConv` layer $l$ with respect message type $m$. Further unpacking the forumla we have:\n","- $W_s^{(l)[m]}$ - linear transformation matrix for the messages of neighboring source nodes of type $s$ along message type $m$.\n","- $W_d^{(l)[m]}$ - linear transformation matrix for the message from the node $v$ itself of type $d$.\n","- $W^{(l)[m]}$ - linear transformation matrix for the concatenated messages from neighboring node's and the central node.\n","- $h_u^{(l-1)}$ - the hidden embedding representation for node $u$ after the $(l-1)$th `HeteroGNNWrapperConv` layer. Note, that this embedding is not associated with a particular message type (see layer diagrams above). \n","- $N_{m}(v)$ - the set of neighbor source nodes $s$ for the node v that we are embedding along message type $m = (s, r, d)$. \n","\n","**NOTE**: We emphasize that each weight matrix is associated with a specific message type $[m]$ and additionally, the weight matrices applied to node messages are differentiated by node type (i.e. $W_s$ and $W_d$).\n","\n","Lastly, for simplicity, we use mean aggregations for $AGG$ where:\n","\n","\\begin{equation}\n","AGG(\\{h_u^{(l-1)}, \\forall u \\in N_{m}(v) \\}) = \\frac{1}{|N_{m}(v)|} \\sum_{u\\in N_{m}(v)} h_u^{(l-1)}\n","\\end{equation}"],"metadata":{"id":"bTtKonV-OIxh"}},{"cell_type":"code","source":["class HeteroGNNConv(pyg_nn.MessagePassing):\n","    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n","        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n","\n","        self.in_channels_src = in_channels_src\n","        self.in_channels_dst = in_channels_dst\n","        self.out_channels = out_channels\n","\n","        # To simplify implementation, please initialize both self.lin_dst\n","        # and self.lin_src out_features to out_channels\n","        self.lin_dst = None\n","        self.lin_src = None\n","\n","        self.lin_update = None\n","\n","        self.lin_src = nn.Linear(self.in_channels_src, self.out_channels)  # W_s^{(l)[m]}\n","        self.lin_dst = nn.Linear(self.in_channels_dst, self.out_channels)  # W_d^{(l)[m]}\n","        self.lin_update = nn.Linear(self.out_channels * 2, self.out_channels)  # W^{(l)[m]}\n","        # input of concat(dst, src) is out_channels + out_channels, while desired output=out_channels\n","\n","    def forward(\n","        self,\n","        node_feature_src,\n","        node_feature_dst,\n","        edge_index,  # sparse adj matrix (edge_index)\n","        size=None\n","    ):\n","        out = self.propagate(edge_index, size=size, node_feature_src=node_feature_src,\n","                             node_feature_dst=node_feature_dst)\n","        return out\n","\n","    def message_and_aggregate(self, edge_index, node_feature_src):\n","\n","        out = None\n","        ## Note:\n","        ## 1. Different from what we implemented in Colabs 3 and 4, we use message_and_aggregate\n","        ##    to combine the previously seperate message and aggregate functions. \n","        ##    The benefit is that we can avoid materializing x_i and x_j\n","        ##    to make the implementation more efficient.\n","        ## 2. To implement efficiently, refer to PyG documentation for message_and_aggregate\n","        ##    and sparse-matrix multiplication:\n","        ##    https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n","        ## 3. Here edge_index is torch_sparse SparseTensor. Although interesting, you\n","        ##    do not need to deeply understand SparseTensor represenations!\n","        ## 4. Conceptually, think through how the message passing and aggregation\n","        ##    expressed mathematically can be expressed through matrix multiplication.\n","\n","        \n","        # edge_index sparse tensor\n","        # apply an efficient matrix multiplication between a sparse adjacency matrix (only {0,1} values)\n","        # and all the src node_type neighbors hidden representation (-> summation),\n","        # and aggregate to get the mean value\n","        out = matmul(edge_index, node_feature_src, reduce=self.aggr)  # \n","\n","        ##########################################\n","\n","        return out\n","\n","    def update(self, aggr_out, node_feature_dst):\n","\n","        \n","\n","        # aggr_out is the output of message_and_aggregate function \n","        aggr_out = self.lin_src(aggr_out)\n","        dst_msg = self.lin_dst(node_feature_dst)\n","        msgs = torch.concat([dst_msg, aggr_out], axis=-1)\n","        # msgs = torch.concat([aggr_out, dst_msg], axis=-1)\n","        aggr_out = self.lin_update(msgs)\n","        # final output is h_v^{(l)[m]}\n","\n","        return aggr_out"],"metadata":{"id":"0etWZB9IOI9o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Heterogeneous GNN Wrapper Layer\n","\n","After implementing the `HeteroGNNConv` layer for each message type, we need to manage and aggregate the node embedding results (with respect to each message types). Here we will implement two types of message type level aggregation.\n","\n","The first one is simply mean aggregation over message types:\n","\n","\\begin{equation}\n","h_v^{(l)} = \\frac{1}{M}\\sum_{m=1}^{M}h_v^{(l)[m]}\n","\\end{equation}\n","\n","where node $v$ has node type $d$ and we sum over the $M$ message types that have destination node type $d$. From our original example, for a node v of type $b$ we aggregate v's `HeteroGNNConv` embeddings for message types $m_2$ and $m_3$ (i.e. $h_v^{(l)[m_2]}$ and $h_v^{(l)[m_3]}$).\n","\n","The second method we implement is the semantic level attention introduced in **HAN** ([Wang et al. (2019)](https://arxiv.org/abs/1903.07293)). Instead of directly averaging on the message type aggregation results, we use attention to learn which message type result is more important, then aggregate across all the message types. Below are the equations for semantic level attention:\n","\n","\\begin{equation}\n","e_{m} = \\frac{1}{|V_{d}|} \\sum_{v \\in V_{d}} q_{attn}^T \\cdot tanh \\Big( W_{attn}^{(l)} \\cdot h_v^{(l)[m]} + b \\Big)\n","\\end{equation}\n","\n","where $m$ is the message type and $d$ refers to the destination node type for that message ($m = (s, r, d)$). Additionally, $V_{d}$ refers to the set of nodes v with type $d$. Lastly, the unormalized attention weight $e_m$ is a scaler computed for each message type $m$. \n","\n","Next, we can compute the normalized attention weights and update $h_v^{(l)}$:\n","\n","\\begin{equation}\n","\\alpha_{m} = \\frac{\\exp(e_{m})}{\\sum_{m=1}^M \\exp(e_{m})}\n","\\end{equation}\n","\n","\\begin{equation}\n","h_v^{(l)} = \\sum_{m=1}^{M} \\alpha_{m} \\cdot h_v^{(l)[m]}\n","\\end{equation}\n","\n",", where we emphasize that $M$ here is the number of message types associated with the destination node type $d$. \n","\n","**Note**: The implementation of the attention aggregation is tricky and nuanced. We strongly recommend working carefully through the math equations to undersatnd exactly what each notation refers to and how all the pieces fit together. If you can, try to connect the math to our original example, focusing on node type $b$, which depends on two different message types!\n","\n","**_We've implemented most of this for you but you'll need to initialize self.attn_proj in the initializer_**"],"metadata":{"id":"5Jc7arl_OH0U"}},{"cell_type":"code","source":["class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n","    def __init__(self, convs, args, aggr=\"mean\"):\n","        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n","        self.aggr = aggr\n","\n","        # Map the index and message type\n","        self.mapping = {}\n","\n","        # A numpy array that stores the final attention probability\n","        self.alpha = None\n","\n","        self.attn_proj = None\n","\n","        if self.aggr == \"attn\":\n","            # attn_proj:    q^T_{attn} * tanh(W^{(l)}_{attn} * h_v^{(l)[m]} + b_{attn})  (one element in e_m)\n","            self.attn_proj = nn.Sequential(\n","                nn.Linear(args['hidden_size'], args['attn_size']),\n","                nn.Tanh(),\n","                nn.Linear(args['attn_size'], 1, bias=False)\n","                )\n","\n","    \n","    def reset_parameters(self):\n","        # reset parent parameters\n","        super(HeteroConvWrapper, self).reset_parameters()   \n","        # reset attention parameters\n","        if self.aggr == \"attn\":\n","            for layer in self.attn_proj.children():\n","                layer.reset_parameters()\n","    \n","    def forward(self, node_features, edge_indices):\n","        message_type_emb = {}\n","        # edge_indices: key= message, value=sparse adj. matrix (represents edge index)\n","        for message_key, message_type in edge_indices.items():\n","            src_type, edge_type, dst_type = message_key\n","            node_feature_src = node_features[src_type]\n","            node_feature_dst = node_features[dst_type]\n","            edge_index = edge_indices[message_key]\n","            message_type_emb[message_key] = (\n","                self.convs[message_key](\n","                    node_feature_src,\n","                    node_feature_dst,\n","                    edge_index,\n","                )\n","            )   # apply HeteroGNNConv for each message type\n","        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}  \n","        #node_emb- key=dst node type, value=list of embeddings of node type 'dst'\n","        mapping = {} \n","        # mapping: key=#node embeddingd with node_type 'dst' , value=message       \n","        for (src, edge_type, dst), item in message_type_emb.items():\n","            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n","            node_emb[dst].append(item)\n","        self.mapping = mapping\n","        for node_type, embs in node_emb.items():\n","            if len(embs) == 1:   # embs- list of embeddings\n","                node_emb[node_type] = embs[0]\n","            else:\n","                node_emb[node_type] = self.aggregate(embs)\n","        return node_emb\n","    \n","    def aggregate(self, xs):\n","        # TODO: Implement this function that aggregates all message type results.\n","        # Here, xs is a list of tensors (embeddings) with respect to message \n","        # type aggregation results.\n","\n","        if self.aggr == \"mean\":\n","\n","            x = torch.stack(xs, dim=-1)  # concatenate list of tensor over last (new) dimension\n","            return x.mean(dim=-1)  # aggregate over the new dim\n","\n","\n","            ##########################################\n","\n","        elif self.aggr == \"attn\":\n","            N = xs[0].shape[0] # Number of nodes for that node type\n","            M = len(xs) # Number of message types for that node type\n","\n","            x = torch.cat(xs, dim=0).view(M, N, -1) # M * N * D\n","            z = self.attn_proj(x).view(M, N) # M * N * 1\n","            z = z.mean(1) # M * 1\n","            alpha = torch.softmax(z, dim=0) # M * 1\n","\n","            # Store the attention result to self.alpha as np array\n","            self.alpha = alpha.view(-1).data.cpu().numpy()\n","  \n","            alpha = alpha.view(M, 1, 1)\n","            x = x * alpha\n","            return x.sum(dim=0)"],"metadata":{"id":"U0zpfIGBH35M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize Heterogeneous GNN Layers\n","\n","Now let's put it all together and initialize the Heterogeneous GNN Layers. Different from the homogeneous graph case, heterogeneous graphs can be a little bit complex.\n","\n","In general, we need to create a dictionary of `HeteroGNNConv` layers where the keys are message types.\n","\n","* To get all message types, `deepsnap.hetero_graph.HeteroGraph.message_types` is useful.\n","* If we are initializing the first conv layers, we need to get the feature dimension of each node type. Using `deepsnap.hetero_graph.HeteroGraph.num_node_features(node_type)` will return the node feature dimension of `node_type`. In this function, we will set each `HeteroGNNConv` `out_channels` to be `hidden_size`.\n","* If we are not initializing the first conv layers, all node types will have the same embedding dimension `hidden_size` and we still set `HeteroGNNConv` `out_channels` to be `hidden_size` for simplicity.\n","\n"],"metadata":{"id":"XqjBUHNFH2g6"}},{"cell_type":"code","source":["def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n","    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n","    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n","    # object and the `conv` is the `HeteroGNNConv`.\n","\n","    convs = {}  # a dictionary with key=msg_tpye, value=HeteroGNNConv layer\n","    msg_types = hetero_graph.message_types  # get list of all messages\n","    \n","    for msg_type in msg_types:\n","        in_channels_src = in_channels_dst = hidden_size\n","        if first_layer:  # extract input size of src node tpye and dst node_type\n","            src_type = msg_type[0]\n","            dst_type = msg_type[-1]  # or 2\n","            in_channels_src = hetero_graph.num_node_features(src_type)\n","            in_channels_dst = hetero_graph.num_node_features(dst_type)\n","        convs[msg_type] = conv(in_channels_src, in_channels_dst, hidden_size)\n","    \n","    return convs"],"metadata":{"id":"K9T1XTVaGWdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HeteroGNN(torch.nn.Module):\n","    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n","        super(HeteroGNN, self).__init__()\n","\n","        self.aggr = aggr\n","        self.hidden_size = args['hidden_size']\n","\n","        self.convs1 = None\n","        self.convs2 = None\n","\n","        self.bns1 = nn.ModuleDict()\n","        self.bns2 = nn.ModuleDict()\n","        self.relus1 = nn.ModuleDict()\n","        self.relus2 = nn.ModuleDict()\n","        self.post_mps = nn.ModuleDict()\n","\n","        convs1 = generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True)\n","        self.convs1 = HeteroGNNWrapperConv(convs1, args, aggr=self.aggr)\n","\n","        convs2 = generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False)\n","        self.convs2 = HeteroGNNWrapperConv(convs2, args, aggr=self.aggr)\n","\n","        for node_type in hetero_graph.node_types:\n","            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=args['eps'])\n","            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=args['eps'])\n","            self.relus1[node_type] = nn.LeakyReLU()\n","            self.relus2[node_type] = nn.LeakyReLU()\n","            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n","      \n","        ##########################################\n","\n","    def forward(self, node_feature, edge_index):\n","        # TODO: Implement the forward function. Notice that `node_feature` is \n","        # a dictionary of tensors where keys are node types and values are \n","        # corresponding feature tensors. The `edge_index` is a dictionary of \n","        # tensors where keys are message types and values are corresponding\n","        # edge index tensors (with respect to each message type).\n","\n","        # forward_op: apply forward pass on dictionaries:\n","            # input:\n","            #   * x- a dictionary with key=node_type, value=tensor of correspondnig nodes' representation\n","            #   * mouledict- a dictionary with key=node_type, value=a module (e.g.glinear/relu/bn layer)\n","\n","\n","        x = node_feature\n","\n","        x = self.convs1(x, edge_index)\n","        x = forward_op(x, self.bns1)   \n","        x = forward_op(x, self.relus1)\n","        x = self.convs2(x, edge_index)\n","        x = forward_op(x, self.bns2)\n","        x = forward_op(x, self.relus2)\n","        x = forward_op(x, self.post_mps)\n","        \n","        return x\n","\n","    def loss(self, preds, y, indices):\n","        \n","        loss = 0\n","        loss_func = F.cross_entropy\n","\n","        for node_type, labeled_idxes in indices.items():\n","        # for each node type take the relevant (supervied) indices of true labels and predictions\n","            y_type = y[node_type][labeled_idxes]\n","            preds_type = preds[node_type][labeled_idxes]\n","            loss += loss_func(preds_type, y_type)\n","        \n","        return loss"],"metadata":{"id":"YGSVLW0J7Hw5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Testing"],"metadata":{"id":"MT1ZHnI6Bs5h"}},{"cell_type":"code","source":["import pandas as pd\n","\n","def train(model, optimizer, hetero_graph, train_idx):\n","    model.train()\n","    optimizer.zero_grad()\n","    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n","\n","    y = hetero_graph.node_label  # a dictionary (node_type, tensor of corresponding nodes' label)\n","    loss = model.loss(preds, y, train_idx)\n","\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()\n","\n","def test(model, graph, indices, best_model=None, best_val=0, save_preds=False, agg_type=None):\n","    model.eval()\n","    accs = []\n","    for i, index in enumerate(indices):\n","        preds = model(graph.node_feature, graph.edge_index)\n","        num_node_types = 0\n","        micro = 0\n","        macro = 0\n","        for node_type in preds:\n","            idx = index[node_type]\n","            pred = preds[node_type][idx]\n","            pred = pred.max(1)[1]  # take the indexes of maximum values (by columns)\n","            label_np = graph.node_label[node_type][idx].cpu().numpy()\n","            pred_np = pred.cpu().numpy()\n","            micro = f1_score(label_np, pred_np, average='micro')\n","            macro = f1_score(label_np, pred_np, average='macro')\n","            num_node_types += 1\n","                  \n","        # Averaging f1 score might not make sense, but in our example we only\n","        # have one node type\n","        micro /= num_node_types\n","        macro /= num_node_types\n","        accs.append((micro, macro))\n","\n","        # Only save the test set predictions and labels!\n","        if save_preds and i == 2:\n","          print (\"Saving Heterogeneous Node Prediction Model Predictions with Agg:\", agg_type)\n","          print()\n","\n","          data = {}\n","          data['pred'] = pred_np\n","          data['label'] = label_np\n","\n","          df = pd.DataFrame(data=data)\n","          # Save locally as csv\n","          df.to_csv('ACM-Node-' + agg_type + 'Agg.csv', sep=',', index=False)\n","\n","    if accs[1][0] > best_val:\n","        best_val = accs[1][0]\n","        best_model = copy.deepcopy(model)\n","    return accs, best_model, best_val"],"metadata":{"id":"8YOxt4pABw_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset and Preprocessing\n","\n","we will load the data and create a tensor backend (without a NetworkX graph) `deepsnap.hetero_graph.HeteroGraph` object.\n","\n","`ACM(3025)` dataset for node property prediction task, which is proposed in **HAN** ([Wang et al. (2019)](https://arxiv.org/abs/1903.07293)) and our dataset is extracted from [DGL](https://www.dgl.ai/)'s [ACM.mat](https://data.dgl.ai/dataset/ACM.mat).\n","\n","The original ACM dataset has three node types and two edge (relation) types. For simplicity, we simplify the heterogeneous graph to one node type and two edge types (shown below). This means that in our heterogeneous graph, we have one node type (paper) and two message types (paper, author, paper) and (paper, subject, paper).\n","\n","<br/>\n","<center>\n","<img src=\"http://web.stanford.edu/class/cs224w/images/colab4/cs224w-acm.png\"/>\n","</center>"],"metadata":{"id":"Ce9pvHK57TEW"}},{"cell_type":"code","source":["args = {\n","    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'hidden_size': 64,\n","    'epochs': 100,\n","    'weight_decay': 1e-5,\n","    'lr': 0.003,\n","    'attn_size': 32,\n","    'eps': 1.0,\n","}"],"metadata":{"id":"-SwhylExBpqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'IS_GRADESCOPE_ENV' not in os.environ:\n","  print(\"Device: {}\".format(args['device']))\n","\n","  # Load the data\n","  data = torch.load(\"acm.pkl\")\n","\n","#   data.keys: dict_keys(['label', 'feature', 'pap', 'psp', 'train_idx', 'val_idx', 'test_idx'])\n","\n","  # Message types\n","  message_type_1 = (\"paper\", \"author\", \"paper\")\n","  message_type_2 = (\"paper\", \"subject\", \"paper\")\n","\n","  # Dictionary of edge indices\n","  edge_index = {}\n","  edge_index[message_type_1] = data['pap']  # 2 X |E|\n","  edge_index[message_type_2] = data['psp']  \n","\n","  # Dictionary of node features\n","  node_feature = {}\n","  node_feature[\"paper\"] = data['feature']   # |V| X D_node\n","\n","  # Dictionary of node labels\n","  node_label = {}\n","  node_label[\"paper\"] = data['label']  #  |V|\n","\n","  # Load the train, validation and test indices\n","  train_idx = {\"paper\": data['train_idx'].to(args['device'])}\n","  val_idx = {\"paper\": data['val_idx'].to(args['device'])}\n","  test_idx = {\"paper\": data['test_idx'].to(args['device'])}\n","\n","  # Construct a deepsnap tensor backend HeteroGraph\n","  hetero_graph = HeteroGraph(\n","      node_feature=node_feature,\n","      node_label=node_label,\n","      edge_index=edge_index,\n","      directed=True\n","  )\n","\n","  print(f\"ACM heterogeneous graph: {hetero_graph.num_nodes()} nodes, {hetero_graph.num_edges()} edges\")\n","\n","  # Node feature and node label to device\n","  for key in hetero_graph.node_feature:\n","      hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n","  for key in hetero_graph.node_label:\n","      hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n","\n","  # Edge_index to sparse tensor (sparse adjacency matrix) and to device\n","  for key in hetero_graph.edge_index:\n","      edge_index = hetero_graph.edge_index[key]\n","      adj = SparseTensor(row=edge_index[0], col=edge_index[1], sparse_sizes=(hetero_graph.num_nodes('paper'), hetero_graph.num_nodes('paper')))\n","      hetero_graph.edge_index[key] = adj.t().to(args['device'])\n","  print(hetero_graph.edge_index[message_type_1])\n","  print(hetero_graph.edge_index[message_type_2])\n","\n","\n","# hetero_graph.edge_index- a dictionary:\n","# * key=message (src node_type, relation, dst node_type)\n","# * value=sparseTensor of edge_index\n","  "],"metadata":{"id":"YcY_o4e_7H0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"3vl3q-us7H27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'IS_GRADESCOPE_ENV' not in os.environ:\n","  best_model = None\n","  best_val = 0\n","\n","  model = HeteroGNN(hetero_graph, args, aggr=\"attn\").to(args['device'])  # model with attention\n","#   model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])  # model with mean aggr\n","  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n","\n","  for epoch in range(args['epochs']):\n","      loss = train(model, optimizer, hetero_graph, train_idx)\n","      accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n","      \n","      print(\n","          f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n","          f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n","          f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n","          f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n","      )\n","\n","  best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx], save_preds=True, agg_type=\"Attention\")\n","#   best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx], save_preds=True, agg_type=\"Mean\")\n","  print(\n","      f\"Best model: \"\n","      f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n","      f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n","      f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n","  )"],"metadata":{"id":"xr5AvRLh7H5S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"xKoAtIsP7H7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Attention for each Message Type\n","\n","Through message type level attention we can learn which message type is more important to which layer.\n","\n","Here we will print out and show that each layer pay how much attention on each message type."],"metadata":{"id":"WUCeGui8GF-L"}},{"cell_type":"code","source":["if 'IS_GRADESCOPE_ENV' not in os.environ:\n","  if model.convs1.alpha is not None and model.convs2.alpha is not None:\n","      for idx, message_type in model.convs1.mapping.items():\n","          print(f\"Layer 1 has attention {model.convs1.alpha[idx]} on message type {message_type}\")\n","      for idx, message_type in model.convs2.mapping.items():\n","          print(f\"Layer 2 has attention {model.convs2.alpha[idx]} on message type {message_type}\")"],"metadata":{"id":"lEwkziMO7H-e"},"execution_count":null,"outputs":[]}]}